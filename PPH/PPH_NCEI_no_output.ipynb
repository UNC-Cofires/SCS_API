{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb985b7-b500-49ad-94cd-6250ff652781",
   "metadata": {},
   "source": [
    "Original storm types\n",
    "storm_types = {\n",
    "    #\"Tornado\": \"torn\",\n",
    "    #\"Thunderstorm Wind\": \"wind\", \n",
    "    #\"Strong Wind\": \"wind\",\n",
    "    #\"High Wind\": \"wind\",\n",
    "    \"Hail\": \"hail\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2293c4-bdee-4cd5-a783-20b7bed8baf6",
   "metadata": {},
   "source": [
    "# Calculate PPH for hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219567d-8d3f-47dc-84d6-ed4376167f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate PPH for annual NCEI_storm_reports\n",
    "1200z Day1 to 1200z Day2 time windows\n",
    "File names use Day1\n",
    "    \n",
    "MODIFIED: Now creates files with all zeros for dates with no events\n",
    "MODIFIED: Accounts for BEGIN_DATE_TIME and END_DATE_TIME with 1200z-1200z periods\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sigma_grid_units = 1.5\n",
    "grid_spacing_km = 40.0   \n",
    "\n",
    "# Download and Load NAM-212 grid \n",
    "url = 'https://github.com/ahaberlie/PPer_Climo/tree/master/data'\n",
    "try:\n",
    "    grid_ds = xr.open_dataset(\"/Users/jimnguyen/IRMII/SCS_API/PPH/nam212.nc\") #Set to your folder pathway\n",
    "    grid212_lat = grid_ds[\"gridlat_212\"].values  # (ny, nx)\n",
    "    grid212_lon = grid_ds[\"gridlon_212\"].values  # (ny, nx)\n",
    "    print(f\"Loaded grid with shape: {grid212_lat.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading grid file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Storm type mapping from EVENT_TYPE\n",
    "storm_types = {\n",
    "    \"Hail\": \"hail\"\n",
    "}\n",
    "\n",
    "# Distance function for PPH\n",
    "def euclidean_distance_km(grid_lat, grid_lon, report_lat, report_lon):\n",
    "    lat_km = 111.32 * (grid_lat - report_lat)\n",
    "    lon_km = 111.32 * np.cos(np.radians(report_lat)) * (grid_lon - report_lon)\n",
    "    return np.sqrt(lat_km**2 + lon_km**2)\n",
    "\n",
    "def parse_datetime_string(dt_string):\n",
    "    \"\"\"Parse datetime string to datetime object\"\"\"\n",
    "    if pd.isna(dt_string) or dt_string == '' or dt_string is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        formats = [\n",
    "            '%d-%b-%y %H:%M:%S',      # 01-APR-24 04:06:00 \n",
    "        ]\n",
    "        \n",
    "        dt_string = str(dt_string).strip()\n",
    "        \n",
    "        # Try each format\n",
    "        for i, fmt in enumerate(formats):\n",
    "            try:\n",
    "                parsed = datetime.strptime(dt_string, fmt)\n",
    "                return parsed\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # If none of the formats work, try to parse with dateutil (more flexible)\n",
    "        # Note: dateutil may not be available in all environments\n",
    "        # try:\n",
    "        #     from dateutil import parser\n",
    "        #     return parser.parse(dt_string)\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def event_overlaps_with_1200z_period(begin_dt, end_dt, period_year, period_month, period_day):\n",
    "    \"\"\"\n",
    "    Check if an event (begin_dt to end_dt) overlaps with a 1200z-1200z period.\n",
    "    Period runs from period_day 1200z to (period_day+1) 1200z.\n",
    "    \"\"\"\n",
    "    if begin_dt is None:\n",
    "        return False\n",
    "    \n",
    "    if end_dt is None:\n",
    "        end_dt = begin_dt  # Assume instantaneous event if no end time\n",
    "    \n",
    "    try:\n",
    "        # Create period boundaries\n",
    "        period_start = datetime(period_year, period_month, period_day, 12, 0)  # 1200z Day1\n",
    "        \n",
    "        # Calculate next day for period end, handling month/year rollover\n",
    "        next_day = period_start + timedelta(days=1)\n",
    "        period_end = next_day.replace(hour=12, minute=0, second=0)  # 1200z Day2\n",
    "        \n",
    "        # Check for overlap: events overlap if event_start < period_end AND event_end > period_start\n",
    "        overlaps = begin_dt < period_end and end_dt > period_start\n",
    "        \n",
    "        return overlaps\n",
    "        \n",
    "    except (ValueError, OverflowError):\n",
    "        return False\n",
    "\n",
    "# Create output directory\n",
    "output_folder = \"NCEI_PPH\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Create output subfolders for each storm type\n",
    "for storm_type in storm_types.values():\n",
    "    output_subfolder = os.path.join(output_folder, storm_type)\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "# Process each year from 2010-2024\n",
    "for year in range(2010, 2025):  #2010 to 2024 inclusive\n",
    "    file_path = f\"/Users/jimnguyen/IRMII/SCS_API/NCEI_storm_reports/hail_filtered/Hail_Reports_{year}.csv\"\n",
    "    \n",
    "    # Initialize data as empty DataFrame in case file doesn't exist\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File does not exist: {file_path} - Will create zero files for all dates\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nProcessing {file_path}...\")\n",
    "            \n",
    "            # Read the data\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Clean and convert data types\n",
    "            data['LAT'] = pd.to_numeric(data['LAT'], errors='coerce')\n",
    "            data['LON'] = pd.to_numeric(data['LON'], errors='coerce')\n",
    "            \n",
    "            # Parse datetime strings\n",
    "            data['BEGIN_DT'] = data['BEGIN_DATE_TIME'].apply(parse_datetime_string)\n",
    "            data['END_DT'] = data['END_DATE_TIME'].apply(parse_datetime_string)\n",
    "            \n",
    "            # Debug datetime parsing\n",
    "            parsed_count = data['BEGIN_DT'].notna().sum()\n",
    "            print(f\"  Successfully parsed datetimes: {parsed_count} / {len(data)}\")\n",
    "            \n",
    "            if parsed_count == 0:\n",
    "                print(\"  ERROR: NO DATETIMES PARSED SUCCESSFULLY!\")\n",
    "                print(\"  Sample datetime strings:\", data['BEGIN_DATE_TIME'].head(3).tolist())\n",
    "                continue  # Skip this year since datetime parsing completely failed\n",
    "            \n",
    "            # Remove rows with missing critical data\n",
    "            initial_count = len(data)\n",
    "            data = data.dropna(subset=['LAT', 'LON', 'BEGIN_DATE_TIME'])\n",
    "            # Also remove rows where datetime parsing failed\n",
    "            data = data[data['BEGIN_DT'].notna()]\n",
    "            \n",
    "            if len(data) < initial_count:\n",
    "                print(f\"  Removed {initial_count - len(data)} rows with missing/invalid data\")\n",
    "\n",
    "            # Filter to CONUS bounds\n",
    "            conus_data = data[(data['LAT'] >= 24.52) & (data['LAT'] <= 49.385) &\n",
    "                             (data['LON'] >= -124.74) & (data['LON'] <= -66.95)]\n",
    "\n",
    "            if len(conus_data) < len(data):\n",
    "                print(f\"  Filtered {len(data) - len(conus_data)} reports outside CONUS\")\n",
    "            \n",
    "            data = conus_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            data = pd.DataFrame()  # Use empty DataFrame if file read fails\n",
    "\n",
    "    # Process each storm type\n",
    "    for event_type, storm_type in storm_types.items():\n",
    "        print(f\"  Processing {event_type} reports for {year}...\")\n",
    "        \n",
    "        # Filter data for this storm type (will be empty if no data loaded)\n",
    "        if len(data) > 0:\n",
    "            # Filter for hail events\n",
    "            storm_data = data[data['EVENT_TYPE'] == event_type].copy() if 'EVENT_TYPE' in data.columns else data.copy()\n",
    "            print(f\"    Found {len(storm_data)} {event_type} reports\")\n",
    "        else:\n",
    "            storm_data = pd.DataFrame()\n",
    "            print(f\"    No data available - creating zero files for all dates\")\n",
    "        \n",
    "        # Get output subfolder for this storm type\n",
    "        output_subfolder = os.path.join(output_folder, storm_type)\n",
    "        \n",
    "        # Process each month (always process all 12 months)\n",
    "        for month in range(1, 13):\n",
    "            # Determine number of days in this month\n",
    "            days_in_month = calendar.monthrange(year, month)[1]\n",
    "            \n",
    "            # Process each day in the month (ALWAYS process all days)\n",
    "            # Each day represents the start of a 1200z-1200z period\n",
    "            for day in range(1, days_in_month + 1):\n",
    "                \n",
    "                # Find all events that overlap with this 1200z-1200z period\n",
    "                period_events = []\n",
    "                \n",
    "                if len(storm_data) > 0:\n",
    "                    for idx, row in storm_data.iterrows():\n",
    "                        if event_overlaps_with_1200z_period(row['BEGIN_DT'], row['END_DT'], year, month, day):\n",
    "                            period_events.append(row)\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                if period_events:\n",
    "                    day_data = pd.DataFrame(period_events)\n",
    "                else:\n",
    "                    day_data = pd.DataFrame()\n",
    "                \n",
    "                # Initialize the sum for PPH (always initialize, even for zero case)\n",
    "                gaussian_sum = np.zeros_like(grid212_lat, dtype=np.float64)\n",
    "\n",
    "                # Compute the PPH if there's data for this period\n",
    "                if len(day_data) > 0:\n",
    "                    for _, row in day_data.iterrows():\n",
    "                        d_km = euclidean_distance_km(\n",
    "                            grid212_lat, grid212_lon,\n",
    "                            row['LAT'], row['LON']\n",
    "                        )\n",
    "                        \n",
    "                        # Convert to grid units \n",
    "                        d_grid = d_km / grid_spacing_km\n",
    "                        \n",
    "                        # Summing the Nth terms \n",
    "                        gaussian_sum += np.exp(-0.5 * (d_grid / sigma_grid_units) ** 2)\n",
    "                    \n",
    "                    print(f\"    Calculated PPH for {storm_type} period {year}-{month:02d}-{day:02d} 1200z to {year}-{month:02d}-{day+1:02d} 1200z ({len(day_data)} reports)\")\n",
    "                else:\n",
    "                    print(f\"    Created zero PPH for {storm_type} period {year}-{month:02d}-{day:02d} 1200z to next day 1200z (0 reports)\")\n",
    "\n",
    "                # Apply prefactor: (1 / (2π σ²)) - this will result in zeros if gaussian_sum is all zeros\n",
    "                gauss_pref = 1.0 / (2.0 * np.pi * sigma_grid_units**2)\n",
    "                daily_pph = gauss_pref * gaussian_sum\n",
    "                rounded_pph = np.round(daily_pph, 10)\n",
    "\n",
    "                # Saving (ALWAYS save, even if all zeros)\n",
    "                # File name uses Day1 of the period\n",
    "                file_name_out = f\"pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "                output_file = os.path.join(output_subfolder, file_name_out)\n",
    "\n",
    "                try:\n",
    "                    df = pd.DataFrame(rounded_pph)\n",
    "                    df.to_csv(output_file, index=False)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error saving PPH for {year}-{month:02d}-{day:02d}: {e}\")\n",
    "                    continue\n",
    "\n",
    "print(\"\\nNOAA Storm Reports PPH processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a293b-cd96-4452-9fd3-f50db526d056",
   "metadata": {},
   "source": [
    "# Original PPH calcuations for all storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b23539-4cde-4bc2-9bd2-c3adf1848258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "\"WIP\"\n",
    "\"Code graphs the nam212_pph and attempts to replicate the Research paper's visualizations\"\n",
    "\"All credit to the original research paper and its code can be found here\"\n",
    "url = 'https://github.com/ahaberlie/PPer_Climot'\n",
    "\n",
    "# Load NAM-212 grid coordinates\n",
    "grid_ds = xr.open_dataset(\"/Users/jimnguyen/IRMII/SCS_API/PPH/nam212.nc\") #Set to your folder pathway\n",
    "lats = grid_ds[\"gridlat_212\"].values\n",
    "lons = grid_ds[\"gridlon_212\"].values\n",
    "\n",
    "# Use Albers Equal Area projection\n",
    "from_proj = ccrs.PlateCarree()\n",
    "projection = ccrs.AlbersEqualArea(central_longitude=-96, central_latitude=37.5, false_easting=0.0, \n",
    "                                 false_northing=0.0, standard_parallels=(29.5, 45.5), globe=None)\n",
    "\n",
    "# Cities to plot\n",
    "cities = {'Denver, CO': (-104.9903, 39.7392),\n",
    "        'Omaha, NE': (-95.9345, 41.2565),\n",
    "        'Columbus, OH': (-82.9988, 39.9612),\n",
    "        'Albany, NY': (-73.7562, 42.6526),\n",
    "        'Charlotte, NC': (-80.8431, 35.2271),\n",
    "        'San Antonio, TX': (-98.4936, 29.4241),\n",
    "        'Oklahoma City, OK': (-97.5164, 35.4676), \n",
    "        'Tuscaloosa, AL': (-87.5692, 33.2098), \n",
    "        'St. Louis, MO': (-90.1994, 38.6270),\n",
    "        'Minneapolis, MN': (-93.2650, 44.9778), \n",
    "        'Orlando, FL': (-81.3792, 28.5383), \n",
    "        'Bismarck, ND': (-100.773703, 46.801942),\n",
    "        'Washington, DC': (-77.0369, 38.9072)}\n",
    "\n",
    "# Maps America\n",
    "def draw_geography(ax):\n",
    "    \"\"\"Add geographic features to the map\"\"\"\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightblue', zorder=9)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=2)\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.8, edgecolor='black', zorder=8)\n",
    "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), edgecolor='black', linewidth=0.8, zorder=9)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale('50m'), facecolor='lightblue', edgecolor='black', linewidth=0.8,zorder=9)\n",
    "    return ax\n",
    "\n",
    "# Creates the key \n",
    "def generate_legend(ax, title, bounds, colors, fontsize=13, propsize=13):\n",
    "    legend_handles = []\n",
    "    \n",
    "    for i in range(len(bounds)):\n",
    "        label = bounds[i]\n",
    "        patch = Patch(facecolor=colors[i], edgecolor='k', label=label)\n",
    "        legend_handles.append(patch)\n",
    "    \n",
    "    cax = ax.legend(handles=legend_handles, framealpha=1, title=title, \n",
    "                   prop={'size': propsize}, ncol=3, loc=3)\n",
    "    cax.set_zorder(10)\n",
    "    cax.get_frame().set_edgecolor('k')\n",
    "    cax.get_title().set_fontsize('{}'.format(fontsize))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Visualizes the PPHs\n",
    "def draw_pper_map(pper_subset, map_title, map_color_scale, map_colors):\n",
    "    cmap = ListedColormap(map_colors)\n",
    "    norm = BoundaryNorm(map_color_scale, ncolors=cmap.N)\n",
    "    \n",
    "    ax = plt.subplot(1, 1, 1, projection=projection)\n",
    "    ax.set_extent([-120, -73, 18.5, 52.5], crs=from_proj)\n",
    "    ax = draw_geography(ax)\n",
    "    \n",
    "    # Create CONUS mask - values outside CONUS will be masked\n",
    "    conus_mask = ((lats >= 24.52) & (lats <= 49.385) & \n",
    "                  (lons >= -124.74) & (lons <= -66.95))\n",
    "    \n",
    "    # Mask both zero values AND values outside CONUS\n",
    "    res = np.ma.masked_where((pper_subset.values == 0) | (~conus_mask), pper_subset.values)\n",
    "    \n",
    "    mmp = ax.pcolormesh(lons, lats, res, zorder=6, \n",
    "                       cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add state lines above the data with more visible black color\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), linewidth=1.0, edgecolor='black', zorder=7)\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(map_color_scale)-1):\n",
    "        val1 = map_color_scale[i]\n",
    "        labels.append(\"≥ {}\".format(val1))\n",
    "    \n",
    "    legend_handles = generate_legend(ax, map_title, labels, map_colors, fontsize=25, propsize=25)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Calculates the mean annual event days\n",
    "def calculate_mean_annual_days(storm_type, severity, start_year, end_year):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    days_processed = 0 \n",
    "    total_days_above_threshold = None\n",
    "    \n",
    "    current_date = start_date \n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "\n",
    "        csv_path = f\"ncei_pph/{storm_type}/pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)       \n",
    "            pph_daily = (df.values >= severity).astype(int)\n",
    "\n",
    "            if total_days_above_threshold is None:\n",
    "                total_days_above_threshold = np.zeros_like(pph_daily)\n",
    "            \n",
    "            total_days_above_threshold += pph_daily\n",
    "            days_processed += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_path}: {e}\")\n",
    "            \n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    if days_processed == 0:\n",
    "        print(f\"No data found for {storm_type} from {start_year} to {end_year}\")\n",
    "        return None\n",
    "        \n",
    "    num_years = end_year - start_year + 1\n",
    "    mean_annual_days = total_days_above_threshold / num_years\n",
    "    print(f\"Processed {days_processed} days for {storm_type}, {num_years} years\")\n",
    "    return mean_annual_days\n",
    "\n",
    "# Main plotting function\n",
    "def plot_pph_analysis(start_year, end_year, storm_configs, output_dir=\"Mean_Annual_Event_Days_Graphs\"):\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory: {output_dir}\")\n",
    "    \n",
    "    # Set up figure parameters\n",
    "    plt.rcParams['figure.figsize'] = 15, 15\n",
    "    \n",
    "    # Label positions\n",
    "    plab_x = .025\n",
    "    plab_y = .95\n",
    "    maxlab_x = .025\n",
    "    maxlab_y = .24\n",
    "    \n",
    "    # Color schemes and scales for each storm type\n",
    "    color_schemes = {\n",
    "        'torn': ['#ffffff','#fdd49e','#fdbb84','#fc8d59','#e34a33','#b30000'],\n",
    "        'hail': ['#ffffff','#d9f0a3','#addd8e','#78c679','#41ab5d','#238443'],  \n",
    "        'wind': ['#ffffff','#c6dbef','#9ecae1','#6baed6','#3182bd','#08519c']\n",
    "    }\n",
    "    \n",
    "    # Scales adjusted for PPH values\n",
    "    scales = {\n",
    "        'torn': {\n",
    "            0.05: [0, 0.1, 0.5, 1.0, 2.0, 5.0, 100],\n",
    "            0.15: [0, 0.5, 1.0, 2.0, 4.0, 8.0, 100],\n",
    "            0.30: [0, 0.2, 0.5, 1.0, 2.0, 4.0, 100]\n",
    "        },\n",
    "        'hail': {\n",
    "            0.05: [0, 0.5, 1.0, 2.0, 4.0, 8.0, 100],\n",
    "            0.15: [0, 1.0, 2.0, 4.0, 8.0, 12.0, 100],\n",
    "            0.30: [0, 0.5, 1.0, 2.0, 4.0, 6.0, 100]\n",
    "        },\n",
    "        'wind': {\n",
    "            0.05: [0, 1.0, 2.0, 4.0, 8.0, 12.0, 100],\n",
    "            0.15: [0, 2.0, 4.0, 8.0, 12.0, 16.0, 100],\n",
    "            0.30: [0, 1.0, 2.0, 4.0, 6.0, 8.0, 100]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    storm_names = {\n",
    "        'torn': 'Tornado',\n",
    "        'hail': 'Hail', \n",
    "        'wind': 'Wind'\n",
    "    }\n",
    "    \n",
    "    # Process each storm type\n",
    "    for storm_type, severities in storm_configs.items():\n",
    "        dy_colors = color_schemes[storm_type]\n",
    "        \n",
    "        for severity in severities:\n",
    "            title = \"Mean Annual Event Days\"\n",
    "            name = f\"{severity*100:.0f}%\"\n",
    "            key = f'{name} {storm_names[storm_type]}'\n",
    "            \n",
    "            # Get appropriate scale\n",
    "            if severity in scales[storm_type]:\n",
    "                pper_scale = scales[storm_type][severity]\n",
    "            else:\n",
    "                # Fallback scale\n",
    "                pper_scale = [0, 0.5, 1.0, 2.0, 4.0, 8.0, 100]\n",
    "            \n",
    "            print(f\"Processing {storm_type} at {severity*100}% severity...\")\n",
    "            data = calculate_mean_annual_days(storm_type, severity, start_year, end_year)\n",
    "            \n",
    "            if data is not None:\n",
    "                # Convert to xarray DataArray\n",
    "                dsub = xr.DataArray(data, dims=['y', 'x'])\n",
    "                \n",
    "                # Find maximum locations\n",
    "                max_val = np.nanmax(dsub.values)\n",
    "                y_max, x_max = np.where(dsub.values == max_val)\n",
    "                \n",
    "                print(f\"Maximum value: {max_val:.3f} at {len(y_max)} locations\")\n",
    "                \n",
    "                # Create the map\n",
    "                fig = plt.figure(figsize=(15, 15))\n",
    "                ax = draw_pper_map(dsub, title, pper_scale, dy_colors)\n",
    "\n",
    "                # Mark maximum locations with larger, bolder crosses\n",
    "                for i in range(len(y_max)):\n",
    "                    ax.plot(lons[y_max[i], x_max[i]], lats[y_max[i], x_max[i]], \"k+\", \n",
    "                           mew=3, ms=20, transform=ccrs.PlateCarree(), zorder=20)\n",
    "                \n",
    "                # Add cities with larger markers\n",
    "                for city_name, city_loc in cities.items():\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'w.', markersize=20, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'k.', markersize=13, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    \n",
    "                # Add text labels\n",
    "                txt = ax.text(plab_x, plab_y, key + \" ({} - {})\".format(start_year, end_year), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "                \n",
    "\n",
    "                txt = ax.text(maxlab_x, maxlab_y, \"Max (+): {:.2f}\".format(float(max_val)), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "                \n",
    "                # Create filename and save image\n",
    "                filename = f\"{storm_type}_{severity*100:.0f}pct_{start_year}-{end_year}.png\"\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                plt.savefig(filepath, dpi=300, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "                print(f\"Saved: {filepath}\")\n",
    "                \n",
    "                plt.close()\n",
    "            else:\n",
    "                print(f\"No data available for {storm_type} at {severity*100}% threshold\")\n",
    "\n",
    "# Storm thresholds\n",
    "storm_configs = {\n",
    "    'torn': [0.05, 0.10, 0.15, 0.30, 0.60],    # 5% threshold\n",
    "    'wind': [0.05, 0.10, 0.15, 0.30, 0.60],    # 15% threshold  \n",
    "    'hail': [0.05, 0.10, 0.15, 0.30, 0.60]     # 15% threshold\n",
    "}\n",
    "\n",
    "start_year = 2010 \n",
    "end_year = 2024\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    plot_pph_analysis(start_year, end_year, storm_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66746b0c-2987-47b3-a0e6-585a07b12ce9",
   "metadata": {},
   "source": [
    "# Graph Hail + other storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7765e3-e680-4688-ab63-d1b4a8948ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "\"Code graphs PPH for both noaa and ncei and attempts to replicate \"\n",
    "\"Practically Perfect Hindcasts of Severe Convective Storms  visualizations\"\n",
    "\n",
    "\"We use NCEI up until year 2024, and then 2025 onwards we use NOAA\"\n",
    "\n",
    "\"All credit to the original research paper and its code can be found here\"\n",
    "url = 'https://github.com/ahaberlie/PPer_Climo'\n",
    "\n",
    "\"Before running, you must have adjusted the following \"\n",
    "\"1) set the correct path for the grid spacing file (grid_ds)\"\n",
    "\"2) Adjust file name for ncei_pph_namXXX output file in get_data_path function\" \n",
    "\"3) Adjust file name for noaa_pph_namXXX output file in get_data_path function\"\n",
    "\"4) Adjust 'scales' in plot_pph_analysis function\"\n",
    "\n",
    "# Load your desired grid coordinates\n",
    "grid_ds = xr.open_dataset(\"/Users/jimnguyen/IRMII/SCS_API/PPH/nam212.nc\") #Set to your folder pathway\n",
    "lats = grid_ds[\"gridlat_212\"].values\n",
    "lons = grid_ds[\"gridlon_212\"].values\n",
    "\n",
    "# Years to analyze \n",
    "start_year = 2010 # Starts in January of this year\n",
    "end_year = 2024 # Ends in December of this year\n",
    "\n",
    "#Adjust file names to match your grid size\n",
    "def get_data_path(storm_type, year, month, day):\n",
    "    if year <= 2024:\n",
    "        return f\"/Users/jimnguyen/IRMII/SCS_API/PPH/NCEI_PPH/{storm_type}/pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "    else:  # year >= 2025\n",
    "        return f\"noaa_pph_outputs/noaa_pph_nam212/{storm_type}/pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "\n",
    "# Choose if \"slight\" or \"moderate\"\n",
    "storm_configs = {\n",
    "    'torn': [0.05, 0.10, 0.15, 0.30, 0.60],    # .05 = slight, .3 = moderate\n",
    "    'wind': [0.05, 0.10, 0.15, 0.30, 0.60],    # .15 = slight, .6 = moderate\n",
    "    'hail': [0.05, 0.10, 0.15, 0.30, 0.60]     # .15 = slight, .6 = moderate\n",
    "}\n",
    "\n",
    "# Use Albers Equal Area projection\n",
    "from_proj = ccrs.PlateCarree()\n",
    "projection = ccrs.AlbersEqualArea(central_longitude=-96, central_latitude=37.5, false_easting=0.0, \n",
    "                                 false_northing=0.0, standard_parallels=(29.5, 45.5), globe=None)\n",
    "\n",
    "# Cities plotted\n",
    "cities = {'Denver, CO': (-104.9903, 39.7392),\n",
    "        'Omaha, NE': (-95.9345, 41.2565),\n",
    "        'Charlotte, NC': (-80.8431, 35.2271),\n",
    "        'San Antonio, TX': (-98.4936, 29.4241),\n",
    "        'Dallas, TX': (-96.7977, 32.7815),\n",
    "        'Oklahoma City, OK': (-97.5164, 35.4676), \n",
    "        'St. Louis, MO': (-90.1994, 38.6270),\n",
    "        'Minneapolis, MN': (-93.2650, 44.9778), \n",
    "        'Bismarck, ND': (-100.773703, 46.801942),\n",
    "        'Chicago ,IL' : (-87.3954, 41.520480),\n",
    "        'Washington, DC': (-77.0369, 38.9072)}\n",
    "\n",
    "# Maps America\n",
    "def draw_geography(ax):\n",
    "    \"\"\"Add geographic features to the map\"\"\"\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightblue', zorder=9)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=2)\n",
    "\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.8, edgecolor='black', zorder=8)\n",
    "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), edgecolor='black', linewidth=0.8, zorder=9)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale('50m'), facecolor='lightblue', edgecolor='black', linewidth=0.8,zorder=9)\n",
    "    return ax\n",
    "\n",
    "# Creates the key \n",
    "def generate_legend(ax, title, bounds, colors, fontsize=13, propsize=13):\n",
    "    legend_handles = []\n",
    "    \n",
    "    for i in range(len(bounds)):\n",
    "        label = bounds[i]\n",
    "        patch = Patch(facecolor=colors[i], edgecolor='k', label=label)\n",
    "        legend_handles.append(patch)\n",
    "    \n",
    "    cax = ax.legend(handles=legend_handles, framealpha=1, title=title, \n",
    "                   prop={'size': propsize}, ncol=3, loc=3)\n",
    "    cax.set_zorder(10)\n",
    "    cax.get_frame().set_edgecolor('k')\n",
    "    cax.get_title().set_fontsize('{}'.format(fontsize))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Visualizes the PPHs\n",
    "def draw_pper_map(pper_subset, map_title, map_color_scale, map_colors):\n",
    "    cmap = ListedColormap(map_colors)\n",
    "    norm = BoundaryNorm(map_color_scale, ncolors=cmap.N)\n",
    "    \n",
    "    ax = plt.subplot(1, 1, 1, projection=projection)\n",
    "    ax.set_extent([-120, -73, 18.5, 52.5], crs=from_proj)\n",
    "    ax = draw_geography(ax)\n",
    "    \n",
    "    # Create CONUS mask - values outside CONUS will be masked\n",
    "    conus_mask = ((lats >= 24.52) & (lats <= 49.385) & \n",
    "                  (lons >= -124.74) & (lons <= -66.95))\n",
    "    \n",
    "    # Mask both zero values AND values outside CONUS\n",
    "    res = np.ma.masked_where((pper_subset.values == 0) | (~conus_mask), pper_subset.values)\n",
    "    \n",
    "    mmp = ax.pcolormesh(lons, lats, res, zorder=6, \n",
    "                       cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add state lines above the data\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), linewidth=1.0, edgecolor='black', zorder=7)\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(map_color_scale)-1):\n",
    "        val1 = map_color_scale[i]\n",
    "        labels.append(\"≥ {}\".format(val1))\n",
    "    \n",
    "    legend_handles = generate_legend(ax, map_title, labels, map_colors, fontsize=25, propsize=25)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Calculates the mean annual event days\n",
    "def calculate_mean_annual_days(storm_type, severity, start_year, end_year):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    days_processed = 0 \n",
    "    total_days_above_threshold = None\n",
    "    ncei_days = 0\n",
    "    noaa_days = 0\n",
    "    \n",
    "    current_date = start_date \n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "\n",
    "        csv_path = get_data_path(storm_type, year, month, day)\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)       \n",
    "            pph_daily = (df.values >= severity).astype(int)\n",
    "\n",
    "            if total_days_above_threshold is None:\n",
    "                total_days_above_threshold = np.zeros_like(pph_daily)\n",
    "            \n",
    "            total_days_above_threshold += pph_daily\n",
    "            days_processed += 1\n",
    "            \n",
    "            # Tracking if NCEI or NOAA\n",
    "            if year <= 2024:\n",
    "                ncei_days += 1\n",
    "            else:\n",
    "                noaa_days += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_path}: {e}\")\n",
    "            \n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    if days_processed == 0:\n",
    "        print(f\"No data found for {storm_type} from {start_year} to {end_year}\")\n",
    "        return None\n",
    "        \n",
    "    num_years = end_year - start_year + 1\n",
    "    mean_annual_days = total_days_above_threshold / num_years\n",
    "    print(f\"Processed {days_processed} days for {storm_type}, {num_years} years\")\n",
    "    print(f\"  - NCEI days: {ncei_days}\")\n",
    "    print(f\"  - NOAA days: {noaa_days}\")\n",
    "    return mean_annual_days\n",
    "\n",
    "# Main plotting function\n",
    "def plot_pph_analysis(start_year, end_year, storm_configs):\n",
    "    \n",
    "    # Set up figure parameters\n",
    "    plt.rcParams['figure.figsize'] = 15, 15\n",
    "    \n",
    "    # Label positions\n",
    "    plab_x = .025\n",
    "    plab_y = .95\n",
    "    maxlab_x = .025\n",
    "    maxlab_y = .24\n",
    "    \n",
    "    # Color schemes and scales for each storm type\n",
    "    color_schemes = {\n",
    "        'torn': ['#ffffff','#fdd49e','#fdbb84','#fc8d59','#e34a33','#b30000'],\n",
    "        'hail': ['#ffffff','#d9f0a3','#addd8e','#78c679','#41ab5d','#238443'],  \n",
    "        'wind': ['#ffffff','#c6dbef','#9ecae1','#6baed6','#3182bd','#08519c']\n",
    "    }\n",
    "    \n",
    "    # Adjust scales depending on your grid type. \n",
    "    # Currently below is grid sizes for NAM212 40km \n",
    "    scales = {\n",
    "        'torn': {\n",
    "            0.05: [0.025, .5, 2, 3, 4, 5, 100],\n",
    "            0.30: [0.0125, 0.15, .3, .45, .6, .75, 100]\n",
    "        },\n",
    "        'hail': {\n",
    "            0.15:  [0.0125, .5, 4, 8, 12, 15, 100],\n",
    "            0.60: [0.0125, .5, 1, 2, 3, 4, 100]\n",
    "\n",
    "        },\n",
    "        'wind': {\n",
    "            0.15:  [0.025, 1, 3, 7, 12, 17, 100],\n",
    "            0.60: [0.0125, .5, 1, 2, 2.5, 3, 100]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    storm_names = {\n",
    "        'torn': 'Tornado',\n",
    "        'hail': 'Hail', \n",
    "        'wind': 'Wind'\n",
    "    }\n",
    "    \n",
    "    # Process each storm type\n",
    "    for storm_type, severities in storm_configs.items():\n",
    "        dy_colors = color_schemes[storm_type]\n",
    "        \n",
    "        for severity in severities:\n",
    "            title = \"Mean Annual Event Days\"\n",
    "            name = f\"{severity*100:.0f}%\"\n",
    "            key = f'{name} {storm_names[storm_type]}'\n",
    "            \n",
    "            # Get appropriate scale\n",
    "            if severity in scales[storm_type]:\n",
    "                pper_scale = scales[storm_type][severity]\n",
    "            else:\n",
    "                # Fallback scale\n",
    "                pper_scale = [0, 0.5, 1.0, 2.0, 4.0, 8.0, 100]\n",
    "            \n",
    "            print(f\"Processing {storm_type} at {severity*100}% severity...\")\n",
    "            data = calculate_mean_annual_days(storm_type, severity, start_year, end_year)\n",
    "            \n",
    "            if data is not None:\n",
    "                dsub = xr.DataArray(data, dims=['y', 'x'])\n",
    "                \n",
    "                # Find maximum locations\n",
    "                max_val = np.nanmax(dsub.values)\n",
    "                y_max, x_max = np.where(dsub.values == max_val)\n",
    "                \n",
    "                print(f\"Maximum value: {max_val:.3f} at {len(y_max)} locations\")\n",
    "                \n",
    "                # Create the map\n",
    "                fig = plt.figure(figsize=(15, 15))\n",
    "                ax = draw_pper_map(dsub, title, pper_scale, dy_colors)\n",
    "\n",
    "                # Mark maximum locations\n",
    "                for i in range(len(y_max)):\n",
    "                    ax.plot(lons[y_max[i], x_max[i]], lats[y_max[i], x_max[i]], \"k+\", \n",
    "                           mew=3, ms=20, transform=ccrs.PlateCarree(), zorder=20)\n",
    "                \n",
    "                # Add cities with markers\n",
    "                for city_name, city_loc in cities.items():\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'w.', markersize=20, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'k.', markersize=13, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    \n",
    "                # Add text labels\n",
    "                txt = ax.text(plab_x, plab_y, key + \" ({} - {})\".format(start_year, end_year), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "                \n",
    "                txt = ax.text(maxlab_x, maxlab_y, \"Max (+): {:.2f}\".format(float(max_val)), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "\n",
    "\n",
    "                filename = f\"{storm_type}_{severity*100:.0f}pct_{start_year}-{end_year}.png\"\n",
    "                \n",
    "                output_dir = f\"/Users/jimnguyen/IRMII/SCS_API/PPH/Mean_Annual_Event_Days_Graphs\"\n",
    "                \n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                else:\n",
    "                    print(\"Output dir already exists\")\n",
    "                    \n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                plt.savefig(filepath, dpi=300, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "            \n",
    "                print(f\"Saved: {filepath}\")\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                print(f\"No data available for {storm_type} at {severity*100}% threshold\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    plot_pph_analysis(start_year, end_year, storm_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad718475-5acf-47a7-9d82-dd1af9545477",
   "metadata": {},
   "source": [
    "# Calculate sighail PPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035405b3-297d-4054-bb23-2bd2feb97014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate PPH for annual NCEI_storm_reports\n",
    "1200z Day1 to 1200z Day2 time windows\n",
    "File names use Day1\n",
    "    \n",
    "MODIFIED: Now creates files with all zeros for dates with no events\n",
    "MODIFIED: Accounts for BEGIN_DATE_TIME and END_DATE_TIME with 1200z-1200z periods\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sigma_grid_units = 1.5\n",
    "grid_spacing_km = 40.0   \n",
    "\n",
    "# Download and Load NAM-212 grid \n",
    "url = 'https://github.com/ahaberlie/PPer_Climo/tree/master/data'\n",
    "try:\n",
    "    grid_ds = xr.open_dataset(\"/Users/jimnguyen/IRMII/SCS_API/PPH/nam212.nc\") #Set to your folder pathway\n",
    "    grid212_lat = grid_ds[\"gridlat_212\"].values  # (ny, nx)\n",
    "    grid212_lon = grid_ds[\"gridlon_212\"].values  # (ny, nx)\n",
    "    print(f\"Loaded grid with shape: {grid212_lat.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading grid file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Storm type mapping from EVENT_TYPE\n",
    "storm_types = {\n",
    "    \"Hail\": \"sighail\"\n",
    "}\n",
    "\n",
    "# Distance function for PPH\n",
    "def euclidean_distance_km(grid_lat, grid_lon, report_lat, report_lon):\n",
    "    lat_km = 111.32 * (grid_lat - report_lat)\n",
    "    lon_km = 111.32 * np.cos(np.radians(report_lat)) * (grid_lon - report_lon)\n",
    "    return np.sqrt(lat_km**2 + lon_km**2)\n",
    "\n",
    "def parse_datetime_string(dt_string):\n",
    "    \"\"\"Parse datetime string to datetime object\"\"\"\n",
    "    if pd.isna(dt_string) or dt_string == '' or dt_string is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        formats = [\n",
    "            '%d-%b-%y %H:%M:%S',      # 01-APR-24 04:06:00 \n",
    "        ]\n",
    "        \n",
    "        dt_string = str(dt_string).strip()\n",
    "        \n",
    "        # Try each format\n",
    "        for i, fmt in enumerate(formats):\n",
    "            try:\n",
    "                parsed = datetime.strptime(dt_string, fmt)\n",
    "                return parsed\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # If none of the formats work, try to parse with dateutil (more flexible)\n",
    "        # Note: dateutil may not be available in all environments\n",
    "        # try:\n",
    "        #     from dateutil import parser\n",
    "        #     return parser.parse(dt_string)\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def event_overlaps_with_1200z_period(begin_dt, end_dt, period_year, period_month, period_day):\n",
    "    \"\"\"\n",
    "    Check if an event (begin_dt to end_dt) overlaps with a 1200z-1200z period.\n",
    "    Period runs from period_day 1200z to (period_day+1) 1200z.\n",
    "    \"\"\"\n",
    "    if begin_dt is None:\n",
    "        return False\n",
    "    \n",
    "    if end_dt is None:\n",
    "        end_dt = begin_dt  # Assume instantaneous event if no end time\n",
    "    \n",
    "    try:\n",
    "        # Create period boundaries\n",
    "        period_start = datetime(period_year, period_month, period_day, 12, 0)  # 1200z Day1\n",
    "        \n",
    "        # Calculate next day for period end, handling month/year rollover\n",
    "        next_day = period_start + timedelta(days=1)\n",
    "        period_end = next_day.replace(hour=12, minute=0, second=0)  # 1200z Day2\n",
    "        \n",
    "        # Check for overlap: events overlap if event_start < period_end AND event_end > period_start\n",
    "        overlaps = begin_dt < period_end and end_dt > period_start\n",
    "        \n",
    "        return overlaps\n",
    "        \n",
    "    except (ValueError, OverflowError):\n",
    "        return False\n",
    "\n",
    "# Create output directory\n",
    "output_folder = \"Sighail_PPH\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Create output subfolders for each storm type\n",
    "for storm_type in storm_types.values():\n",
    "    output_subfolder = os.path.join(output_folder, storm_type)\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "# Process each year from 2010-2024\n",
    "for year in range(2010, 2025):  #2010 to 2024 inclusive\n",
    "    file_path = f\"/Users/jimnguyen/IRMII/SCS_API/NCEI_storm_reports/sighail_filtered/Sighail_Reports_{year}.csv\"\n",
    "    \n",
    "    # Initialize data as empty DataFrame in case file doesn't exist\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File does not exist: {file_path} - Will create zero files for all dates\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nProcessing {file_path}...\")\n",
    "            \n",
    "            # Read the data\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Clean and convert data types\n",
    "            data['LAT'] = pd.to_numeric(data['LAT'], errors='coerce')\n",
    "            data['LON'] = pd.to_numeric(data['LON'], errors='coerce')\n",
    "            \n",
    "            # Parse datetime strings\n",
    "            data['BEGIN_DT'] = data['BEGIN_DATE_TIME'].apply(parse_datetime_string)\n",
    "            data['END_DT'] = data['END_DATE_TIME'].apply(parse_datetime_string)\n",
    "            \n",
    "            # Debug datetime parsing\n",
    "            parsed_count = data['BEGIN_DT'].notna().sum()\n",
    "            print(f\"  Successfully parsed datetimes: {parsed_count} / {len(data)}\")\n",
    "            \n",
    "            if parsed_count == 0:\n",
    "                print(\"  ERROR: NO DATETIMES PARSED SUCCESSFULLY!\")\n",
    "                print(\"  Sample datetime strings:\", data['BEGIN_DATE_TIME'].head(3).tolist())\n",
    "                continue  # Skip this year since datetime parsing completely failed\n",
    "            \n",
    "            # Remove rows with missing critical data\n",
    "            initial_count = len(data)\n",
    "            data = data.dropna(subset=['LAT', 'LON', 'BEGIN_DATE_TIME'])\n",
    "            # Also remove rows where datetime parsing failed\n",
    "            data = data[data['BEGIN_DT'].notna()]\n",
    "            \n",
    "            if len(data) < initial_count:\n",
    "                print(f\"  Removed {initial_count - len(data)} rows with missing/invalid data\")\n",
    "\n",
    "            # Filter to CONUS bounds\n",
    "            conus_data = data[(data['LAT'] >= 24.52) & (data['LAT'] <= 49.385) &\n",
    "                             (data['LON'] >= -124.74) & (data['LON'] <= -66.95)]\n",
    "\n",
    "            if len(conus_data) < len(data):\n",
    "                print(f\"  Filtered {len(data) - len(conus_data)} reports outside CONUS\")\n",
    "            \n",
    "            data = conus_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {e}\")\n",
    "            data = pd.DataFrame()  # Use empty DataFrame if file read fails\n",
    "\n",
    "    # Process each storm type\n",
    "    for event_type, storm_type in storm_types.items():\n",
    "        print(f\"  Processing {event_type} reports for {year}...\")\n",
    "        \n",
    "        # Filter data for this storm type (will be empty if no data loaded)\n",
    "        if len(data) > 0:\n",
    "            # Filter for hail events\n",
    "            storm_data = data[data['EVENT_TYPE'] == event_type].copy() if 'EVENT_TYPE' in data.columns else data.copy()\n",
    "            print(f\"    Found {len(storm_data)} {event_type} reports\")\n",
    "        else:\n",
    "            storm_data = pd.DataFrame()\n",
    "            print(f\"    No data available - creating zero files for all dates\")\n",
    "        \n",
    "        # Get output subfolder for this storm type\n",
    "        output_subfolder = os.path.join(output_folder, storm_type)\n",
    "        \n",
    "        # Process each month (always process all 12 months)\n",
    "        for month in range(1, 13):\n",
    "            # Determine number of days in this month\n",
    "            days_in_month = calendar.monthrange(year, month)[1]\n",
    "            \n",
    "            # Process each day in the month (ALWAYS process all days)\n",
    "            # Each day represents the start of a 1200z-1200z period\n",
    "            for day in range(1, days_in_month + 1):\n",
    "                \n",
    "                # Find all events that overlap with this 1200z-1200z period\n",
    "                period_events = []\n",
    "                \n",
    "                if len(storm_data) > 0:\n",
    "                    for idx, row in storm_data.iterrows():\n",
    "                        if event_overlaps_with_1200z_period(row['BEGIN_DT'], row['END_DT'], year, month, day):\n",
    "                            period_events.append(row)\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                if period_events:\n",
    "                    day_data = pd.DataFrame(period_events)\n",
    "                else:\n",
    "                    day_data = pd.DataFrame()\n",
    "                \n",
    "                # Initialize the sum for PPH (always initialize, even for zero case)\n",
    "                gaussian_sum = np.zeros_like(grid212_lat, dtype=np.float64)\n",
    "\n",
    "                # Compute the PPH if there's data for this period\n",
    "                if len(day_data) > 0:\n",
    "                    for _, row in day_data.iterrows():\n",
    "                        d_km = euclidean_distance_km(\n",
    "                            grid212_lat, grid212_lon,\n",
    "                            row['LAT'], row['LON']\n",
    "                        )\n",
    "                        \n",
    "                        # Convert to grid units \n",
    "                        d_grid = d_km / grid_spacing_km\n",
    "                        \n",
    "                        # Summing the Nth terms \n",
    "                        gaussian_sum += np.exp(-0.5 * (d_grid / sigma_grid_units) ** 2)\n",
    "                    \n",
    "                    print(f\"    Calculated PPH for {storm_type} period {year}-{month:02d}-{day:02d} 1200z to {year}-{month:02d}-{day+1:02d} 1200z ({len(day_data)} reports)\")\n",
    "                else:\n",
    "                    print(f\"    Created zero PPH for {storm_type} period {year}-{month:02d}-{day:02d} 1200z to next day 1200z (0 reports)\")\n",
    "\n",
    "                # Apply prefactor: (1 / (2π σ²)) - this will result in zeros if gaussian_sum is all zeros\n",
    "                gauss_pref = 1.0 / (2.0 * np.pi * sigma_grid_units**2)\n",
    "                daily_pph = gauss_pref * gaussian_sum\n",
    "                rounded_pph = np.round(daily_pph, 10)\n",
    "\n",
    "                # Saving (ALWAYS save, even if all zeros)\n",
    "                # File name uses Day1 of the period\n",
    "                file_name_out = f\"pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "                output_file = os.path.join(output_subfolder, file_name_out)\n",
    "\n",
    "                try:\n",
    "                    df = pd.DataFrame(rounded_pph)\n",
    "                    df.to_csv(output_file, index=False)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error saving PPH for {year}-{month:02d}-{day:02d}: {e}\")\n",
    "                    continue\n",
    "\n",
    "print(\"\\nPPH processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ccd02-be56-481a-bf67-6f5ec3553de1",
   "metadata": {},
   "source": [
    "# Graph sighail PPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae81ac-44f6-40fc-b4f3-3bef994a2427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "\n",
    "\"Code graphs PPH for both noaa and ncei and attempts to replicate \"\n",
    "\"Practically Perfect Hindcasts of Severe Convective Storms  visualizations\"\n",
    "\n",
    "\"We use NCEI up until year 2024, and then 2025 onwards we use NOAA\"\n",
    "\n",
    "\"All credit to the original research paper and its code can be found here\"\n",
    "url = 'https://github.com/ahaberlie/PPer_Climo'\n",
    "\n",
    "\"Before running, you must have adjusted the following \"\n",
    "\"1) set the correct path for the grid spacing file (grid_ds)\"\n",
    "\"2) Adjust file name for ncei_pph_namXXX output file in get_data_path function\" \n",
    "\"3) Adjust file name for noaa_pph_namXXX output file in get_data_path function\"\n",
    "\"4) Adjust 'scales' in plot_pph_analysis function\"\n",
    "\n",
    "# Load your desired grid coordinates\n",
    "grid_ds = xr.open_dataset(\"/Users/jimnguyen/IRMII/SCS_API/PPH/nam212.nc\") #Set to your folder pathway\n",
    "lats = grid_ds[\"gridlat_212\"].values\n",
    "lons = grid_ds[\"gridlon_212\"].values\n",
    "\n",
    "# Years to analyze \n",
    "start_year = 2010 # Starts in January of this year\n",
    "end_year = 2024 # Ends in December of this year\n",
    "\n",
    "#Adjust file names to match your grid size\n",
    "def get_data_path(storm_type, year, month, day):\n",
    "    if year <= 2024:\n",
    "        return f\"/Users/jimnguyen/IRMII/SCS_API/PPH/Sighail_PPH/sighail/pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "    else:  # year >= 2025\n",
    "        return f\"noaa_pph_outputs/noaa_pph_nam212/{storm_type}/pph_{year}_{month:02d}_{day:02d}.csv\"\n",
    "\n",
    "# Choose if \"slight\" or \"moderate\"\n",
    "storm_configs = {\n",
    "    'sighail': [0.10]\n",
    "    #'sighail': [0.05, 0.10, 0.15, 0.30, 0.60]     # .15 = slight, .6 = moderate\n",
    "\n",
    "}\n",
    "\n",
    "# Use Albers Equal Area projection\n",
    "from_proj = ccrs.PlateCarree()\n",
    "projection = ccrs.AlbersEqualArea(central_longitude=-96, central_latitude=37.5, false_easting=0.0, \n",
    "                                 false_northing=0.0, standard_parallels=(29.5, 45.5), globe=None)\n",
    "\n",
    "# Cities plotted\n",
    "cities = {'Denver, CO': (-104.9903, 39.7392),\n",
    "        'Omaha, NE': (-95.9345, 41.2565),\n",
    "        'Charlotte, NC': (-80.8431, 35.2271),\n",
    "        'San Antonio, TX': (-98.4936, 29.4241),\n",
    "        'Dallas, TX': (-96.7977, 32.7815),\n",
    "        'Oklahoma City, OK': (-97.5164, 35.4676), \n",
    "        'St. Louis, MO': (-90.1994, 38.6270),\n",
    "        'Minneapolis, MN': (-93.2650, 44.9778), \n",
    "        'Bismarck, ND': (-100.773703, 46.801942),\n",
    "        'Chicago ,IL' : (-87.3954, 41.520480),\n",
    "        'Washington, DC': (-77.0369, 38.9072)}\n",
    "\n",
    "# Maps America\n",
    "def draw_geography(ax):\n",
    "    \"\"\"Add geographic features to the map\"\"\"\n",
    "    ax.add_feature(cfeature.OCEAN, color='lightblue', zorder=9)\n",
    "    ax.add_feature(cfeature.LAND, color='darkgray', zorder=2)\n",
    "\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.8, edgecolor='black', zorder=8)\n",
    "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), edgecolor='black', linewidth=0.8, zorder=9)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale('50m'), facecolor='lightblue', edgecolor='black', linewidth=0.8,zorder=9)\n",
    "    return ax\n",
    "\n",
    "# Creates the key \n",
    "def generate_legend(ax, title, bounds, colors, fontsize=13, propsize=13):\n",
    "    legend_handles = []\n",
    "    \n",
    "    for i in range(len(bounds)):\n",
    "        label = bounds[i]\n",
    "        patch = Patch(facecolor=colors[i], edgecolor='k', label=label)\n",
    "        legend_handles.append(patch)\n",
    "    \n",
    "    cax = ax.legend(handles=legend_handles, framealpha=1, title=title, \n",
    "                   prop={'size': propsize}, ncol=3, loc=3)\n",
    "    cax.set_zorder(10)\n",
    "    cax.get_frame().set_edgecolor('k')\n",
    "    cax.get_title().set_fontsize('{}'.format(fontsize))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Visualizes the PPHs\n",
    "def draw_pper_map(pper_subset, map_title, map_color_scale, map_colors):\n",
    "    cmap = ListedColormap(map_colors)\n",
    "    norm = BoundaryNorm(map_color_scale, ncolors=cmap.N)\n",
    "    \n",
    "    ax = plt.subplot(1, 1, 1, projection=projection)\n",
    "    ax.set_extent([-120, -73, 18.5, 52.5], crs=from_proj)\n",
    "    ax = draw_geography(ax)\n",
    "    \n",
    "    # Create CONUS mask - values outside CONUS will be masked\n",
    "    conus_mask = ((lats >= 24.52) & (lats <= 49.385) & \n",
    "                  (lons >= -124.74) & (lons <= -66.95))\n",
    "    \n",
    "    # Mask both zero values AND values outside CONUS\n",
    "    res = np.ma.masked_where((pper_subset.values == 0) | (~conus_mask), pper_subset.values)\n",
    "    \n",
    "    mmp = ax.pcolormesh(lons, lats, res, zorder=6, \n",
    "                       cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add state lines above the data\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), linewidth=1.0, edgecolor='black', zorder=7)\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(map_color_scale)-1):\n",
    "        val1 = map_color_scale[i]\n",
    "        labels.append(\"≥ {}\".format(val1))\n",
    "    \n",
    "    legend_handles = generate_legend(ax, map_title, labels, map_colors, fontsize=25, propsize=25)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Calculates the mean annual event days\n",
    "def calculate_mean_annual_days(storm_type, severity, start_year, end_year):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    days_processed = 0 \n",
    "    total_days_above_threshold = None\n",
    "    ncei_days = 0\n",
    "    noaa_days = 0\n",
    "    \n",
    "    current_date = start_date \n",
    "    while current_date <= end_date:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "\n",
    "        csv_path = get_data_path(storm_type, year, month, day)\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)       \n",
    "            pph_daily = (df.values >= severity).astype(int)\n",
    "\n",
    "            if total_days_above_threshold is None:\n",
    "                total_days_above_threshold = np.zeros_like(pph_daily)\n",
    "            \n",
    "            total_days_above_threshold += pph_daily\n",
    "            days_processed += 1\n",
    "            \n",
    "            # Tracking if NCEI or NOAA\n",
    "            if year <= 2024:\n",
    "                ncei_days += 1\n",
    "            else:\n",
    "                noaa_days += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_path}: {e}\")\n",
    "            \n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    if days_processed == 0:\n",
    "        print(f\"No data found for {storm_type} from {start_year} to {end_year}\")\n",
    "        return None\n",
    "        \n",
    "    num_years = end_year - start_year + 1\n",
    "    mean_annual_days = total_days_above_threshold / num_years\n",
    "    print(f\"Processed {days_processed} days for {storm_type}, {num_years} years\")\n",
    "    print(f\"  - NCEI days: {ncei_days}\")\n",
    "    print(f\"  - NOAA days: {noaa_days}\")\n",
    "    return mean_annual_days\n",
    "\n",
    "# Main plotting function\n",
    "def plot_pph_analysis(start_year, end_year, storm_configs):\n",
    "    \n",
    "    # Set up figure parameters\n",
    "    plt.rcParams['figure.figsize'] = 15, 15\n",
    "    \n",
    "    # Label positions\n",
    "    plab_x = .025\n",
    "    plab_y = .95\n",
    "    maxlab_x = .025\n",
    "    maxlab_y = .24\n",
    "    \n",
    "    # Color schemes and scales for each storm type\n",
    "    color_schemes = {\n",
    "        'sighail': ['#ffffff','#d9f0a3','#addd8e','#78c679','#41ab5d','#238443'],  \n",
    "    }\n",
    "    \n",
    "    # Adjust scales depending on your grid type. \n",
    "    # Currently below is grid sizes for NAM212 40km \n",
    "    scales = {\n",
    "        'sighail': {\n",
    "            0.15:  [0.0125, .5, 4, 8, 12, 15, 100],\n",
    "            0.60: [0.0125, .5, 1, 2, 3, 4, 100]\n",
    "\n",
    "        },\n",
    "    }\n",
    "\n",
    "    storm_names = {\n",
    "        'sighail': 'sighail', \n",
    "    }\n",
    "    \n",
    "    # Process each storm type\n",
    "    for storm_type, severities in storm_configs.items():\n",
    "        dy_colors = color_schemes[storm_type]\n",
    "        \n",
    "        for severity in severities:\n",
    "            title = \"Mean Annual Event Days\"\n",
    "            name = f\"{severity*100:.0f}%\"\n",
    "            key = f'{name} {storm_names[storm_type]}'\n",
    "            \n",
    "            # Get appropriate scale\n",
    "            if severity in scales[storm_type]:\n",
    "                pper_scale = scales[storm_type][severity]\n",
    "            else:\n",
    "                # Fallback scale\n",
    "                pper_scale = [0, 0.5, 1.0, 2.0, 4.0, 8.0, 100]\n",
    "            \n",
    "            print(f\"Processing {storm_type} at {severity*100}% severity...\")\n",
    "            data = calculate_mean_annual_days(storm_type, severity, start_year, end_year)\n",
    "            \n",
    "            if data is not None:\n",
    "                dsub = xr.DataArray(data, dims=['y', 'x'])\n",
    "                \n",
    "                # Find maximum locations\n",
    "                max_val = np.nanmax(dsub.values)\n",
    "                y_max, x_max = np.where(dsub.values == max_val)\n",
    "                \n",
    "                print(f\"Maximum value: {max_val:.3f} at {len(y_max)} locations\")\n",
    "                \n",
    "                # Create the map\n",
    "                fig = plt.figure(figsize=(15, 15))\n",
    "                ax = draw_pper_map(dsub, title, pper_scale, dy_colors)\n",
    "\n",
    "                # Mark maximum locations\n",
    "                for i in range(len(y_max)):\n",
    "                    ax.plot(lons[y_max[i], x_max[i]], lats[y_max[i], x_max[i]], \"k+\", \n",
    "                           mew=3, ms=20, transform=ccrs.PlateCarree(), zorder=20)\n",
    "                \n",
    "                # Add cities with markers\n",
    "                for city_name, city_loc in cities.items():\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'w.', markersize=20, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    ax.plot(city_loc[0], city_loc[1], 'k.', markersize=13, \n",
    "                           transform=from_proj, zorder=10)\n",
    "                    \n",
    "                # Add text labels\n",
    "                txt = ax.text(plab_x, plab_y, key + \" ({} - {})\".format(start_year, end_year), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "                \n",
    "                txt = ax.text(maxlab_x, maxlab_y, \"Max (+): {:.2f}\".format(float(max_val)), \n",
    "                      transform=ax.transAxes, fontsize=25, \n",
    "                      bbox=dict(facecolor='w', edgecolor='k', boxstyle='round'), zorder=15)\n",
    "\n",
    "\n",
    "                filename = f\"{storm_type}_{severity*100:.0f}pct_{start_year}-{end_year}.png\"\n",
    "                \n",
    "                output_dir = f\"/Users/jimnguyen/IRMII/SCS_API/PPH/SIG_HAIL_Mean_Annual_Event_Days_Graphs\"\n",
    "                \n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                else:\n",
    "                    print(\"Output dir already exists\")\n",
    "                    \n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                plt.savefig(filepath, dpi=300, bbox_inches='tight', \n",
    "                           facecolor='white', edgecolor='none')\n",
    "            \n",
    "                print(f\"Saved: {filepath}\")\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                print(f\"No data available for {storm_type} at {severity*100}% threshold\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    plot_pph_analysis(start_year, end_year, storm_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59c0b0-8d62-4da0-adc1-a77a37c8bdc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get PPH graphs for individual years\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "223ef1f7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab6d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
